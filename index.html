<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title -->
  <meta name="title" content="SynShapes: A Synthetic Image-Annotation Dataset for Edge Detection">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="The first dataset composed of synthetic images paired with exact edge map annotations">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Edge Detection, Synthetic data">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="AVNet: Cross-Spectral Attention-Vision Model for Camouflaged Object Detection in Ecological Conservation">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="The first dataset composed of synthetic images paired with exact edge map annotations">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://anonymous.4open.science/w/SynShapes-34F0/">
  <meta property="article:tag" content="Camouflaged Object Detection">
  <meta property="article:tag" content="RGB-Thermal dataset">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="SynShapes: A Synthetic Image-Annotation Dataset for Edge Detection">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="The first dataset composed of synthetic images paired with exact edge map annotations">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <!-- TODO: Replace with your paper title and authors -->
  <title>SynShapes</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/datasetIcon2.ico">
  <link rel="apple-touch-icon" href="static/images/datasetIcon2.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
</head>
  

<body>
  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AVNet: Cross-Spectral Attention-Vision Model for Camouflaged Object
              Detection in Ecological Conservation</h1>
            
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a target="_blank">Henry O. Velesaca</a><sup></sup>,</span>
              <span class="author-block"><a target="_blank">Andrea Mero</a><sup></sup>,</span>
              <span class="author-block"><a target="_blank">Rafael E. Rivadeneira</a>,</span>
              <span class="author-block"><a target="_blank">Guillermo A. Castillo</a>,</span>
              <span class="author-block"><a target="_blank">Angel D. Sappa</a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><br>VISAPP2026</span>
              <!-- <span class="author-block">ISVC Institution Name<br>Conferance name and year</span>  -->
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf -->
                <span class="link-block">
                  <a href="static/pdfs/AVNet__Cross_Spectral_Attention_Vision_Model_for_Camouflaged_Object_Detection_in_Ecological_Conservation___VISAPP_2026.pdf"
                    target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>



                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->

                <span class="link-block">
                  <a href="https://www.kaggle.com/datasets/hvelesaca/iguanadataset" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-download"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/COD-espol/AVNet" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="container-tb1">
          <img src="static/images/AVNet.png" alt="MY ALT TEXT" />
          <h2 class="subtitle has-text-centered">
            <p style="margin-top: 20px;">
              Figure 1. The overall architecture of the proposed AVNet
            </p>
          </h2>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              This work introduced AVNet, a novel attention-vision architecture for Camouflaged Object Detection (COD),
              optimized for ecological conservation. The proposed approach integrates an RGB-Thermal fusion approach with 
              the Convolutional Block Attention Model (CBAM) within an encoder-decoder framework, enabling accurate detection 
              of low-contrast and highly camouflaged targets. As an additional contribution, this study
              introduces the Bimodal Iguana Observational Set (BIOS), comprising 148 camouflaged RGB-Thermal registered image pairs, 
              specifically collected to support COD research in wildlife conservation. Experimental
              results validate the model’s robustness under challenging real-world conditions. The original code and dataset
              presented in the study are openly available in GitHub at 
              <a href="https://anonymous.4open.science/w/AVNet-D75E/">https://cod-espol.github.io/AVNet</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="hero is-small">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="container-tb1">
          <img src="static/images/CharacteristicEvaluatedSOTACOD.png" alt="MY ALT TEXT" />
          <h2 class="subtitle has-text-centered" style="font-size: 18px; margin-top: 20px;">
            Table 1. Distinctive characteristics of the evaluated SOTA COD techniques.
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="container-tb2-tb3">
          <div>
            <img src="static/images/EvaluationMetricsSOTACOD.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered" style="font-size: 18px; margin-top: 20px;">
              Table 2. Experimental results for SOTA COD techniques and AVNet on the BIOS dataset.
              The best three performing results are highlighted using color: <span class="box first">First</span>,
              <span class="box second">Second</span>, and
              <span class="box third">Third</span> respectively.
            </h2>
          </div>
          <!-- <div>
            <img src="static/images/PercentageDifferenceResultsSOTACOD.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Table 3. Percentage difference between the results obtained for each SOTA COD
              technique using the original and re-annotated dataset shown in Table 2. Positive values
              indicate improvement in models using re-annotated labels; negative values indicate a
              deterioration in performance. The last row shows the improvement (%) results of the
              training models using re-annotated dataset concerning the original dataset.
            </h2>
          </div> -->
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="container-tb1">
          <img src="static/images/Predictions_SOTA_COD.png" alt="MY ALT TEXT" />
          <h2 class="subtitle has-text-centered" style="font-size: 18px; margin-top: 20px;">
            Figure 2. Qualitative prediction results using SOTA COD techniques that have achieved first
            place in at least one of the metrics. Successful matches between GT and predicted
            masks (white areas); False positive regions (<span style="color: red;">red</span>
            areas, over-segmentation); and false negative regions (<span style="color: blue;">blue</span>
            areas, miss-segmentation).
          </h2>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small is-light">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="container-tb1">
          <img src="static/images/centroid_positions.png" alt="MY ALT TEXT" />
          <h2 class="subtitle has-text-centered" style="font-size: 18px; margin-top: 20px;">
            Figure 3. Centroid distribution of masks.
          </h2>
        </div>
      </div>
    </div>
  </section>



  <!-- Paper poster -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Paper</h2>

        <iframe
          src="static/pdfs/AVNet__Cross_Spectral_Attention_Vision_Model_for_Camouflaged_Object_Detection_in_Ecological_Conservation___VISAPP_2026.pdf"
          width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <!--<button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>-->
      </div>
      <p>Please cite this paper if you find it helpful (accepted paper),</p>
      <pre id="bibtex-code">
        <code>
          @inproceedings{velesaca2026cod_rgb_thermal,
            title={AVNet: Cross-Spectral Attention-Vision Model for Camouflaged Object Detection in Ecological Conservation},
            author={Velesaca, Henry O. and Mero, Andrea and E. Rivadeneira, Rafael and Castillo, Guillermo A. and Sappa, Angel D.},
            booktitle={Int. Conf. on Computer Vision Theory and Applications, Marbella, Spain},
            pages={1--10},
            year={2026}
          }
        </code>
      </pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>


</html>














